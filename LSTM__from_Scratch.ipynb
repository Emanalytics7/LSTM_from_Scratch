{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRZWcuS4bILQEm5J3aGVzj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Emanalytics7/LSTM_from_Scratch/blob/main/LSTM__from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM model from scratch using Python\n"
      ],
      "metadata": {
        "id": "4NlCaZvEFfyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "PjJqySSCFfnF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Data #####\n",
        "data = \"\"\"To be, or not to be, that is the question: Whether \\\n",
        "'tis nobler in the mind to suffer The slings and arrows of ou\\\n",
        "trageous fortune, Or to take arms against a sea of troubles A\\\n",
        "nd by opposing end them. To die—to sleep, No more; and by a s\\\n",
        "leep to say we end The heart-ache and the thousand natural sh\\\n",
        "ocks That flesh is heir to: 'tis a consummation Devoutly to b\\\n",
        "e wish'd. To die, to sleep; To sleep, perchance to dream—ay, \\\n",
        "there's the rub: For in that sleep of death what dreams may c\\\n",
        "ome, When we have shuffled off this mortal coil, Must give us\\\n",
        " pause—there's the respect That makes calamity of so long lif\\\n",
        "e. For who would bear the whips and scorns of time, Th'oppres\\\n",
        "sor's wrong, the proud man's contumely, The pangs of dispriz'\\\n",
        "d love, the law's delay, The insolence of office, and the spu\\\n",
        "rns That patient merit of th'unworthy takes, When he himself \\\n",
        "might his quietus make\"\"\".lower()\n",
        "chars = set(data)"
      ],
      "metadata": {
        "id": "wwG_S8pyFfkq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_size, char_size = len(data), len(chars)\n",
        "print(data_size, char_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibeim0E8FfiI",
        "outputId": "09a1406f-97ad-4e38-b8db-78bdac1f5336"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "866 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_idx = {c:i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i:c for i, c in enumerate(chars)}\n",
        "\n",
        "char_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-QrQSGZFff7",
        "outputId": "49a7972b-bce2-46ce-ec83-2f1c5ee5e21b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'d': 0,\n",
              " 'n': 1,\n",
              " 'q': 2,\n",
              " ':': 3,\n",
              " '—': 4,\n",
              " 't': 5,\n",
              " 'k': 6,\n",
              " 'c': 7,\n",
              " \"'\": 8,\n",
              " 'z': 9,\n",
              " ',': 10,\n",
              " 'v': 11,\n",
              " 's': 12,\n",
              " 'a': 13,\n",
              " 'm': 14,\n",
              " 'h': 15,\n",
              " 'o': 16,\n",
              " 'e': 17,\n",
              " 'b': 18,\n",
              " 'p': 19,\n",
              " '.': 20,\n",
              " 'f': 21,\n",
              " 'i': 22,\n",
              " 'w': 23,\n",
              " '-': 24,\n",
              " 'u': 25,\n",
              " 'r': 26,\n",
              " ';': 27,\n",
              " ' ': 28,\n",
              " 'g': 29,\n",
              " 'y': 30,\n",
              " 'l': 31}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = data[: -1], data[1:]"
      ],
      "metadata": {
        "id": "Cx9kPMgkFfeg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uniform Xavier Initialization  \n",
        "draw each weight, w, from a random uniform distribution in in [-x,x] for\n",
        "\n",
        "$x = \\sqrt{\\frac{6}{\\text{inputs} + \\text{outputs}}}$"
      ],
      "metadata": {
        "id": "4xvqoOqRYpzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def oneHotEncode(text):\n",
        "    output = np.zeros((char_size, 1)) # 32 rows with 1 column\n",
        "    output[char_to_idx[text]] = 1\n",
        "    return output\n",
        "\n",
        "def initWeights(input_size,  output_size):\n",
        "    return np.random.uniform(-1, 1, (output_size, input_size))  * np.sqrt(6 / (input_size + output_size))"
      ],
      "metadata": {
        "id": "4rXGHPRhFfWD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation function\n",
        "\n",
        "def sigmoid(input, derivative=False):\n",
        "    if derivative:\n",
        "        return input * (1 - input)\n",
        "\n",
        "    return 1 / (1 + np.exp(-input))\n",
        "\n",
        "\n",
        "def tanh(input, derivative = False):\n",
        "    if derivative:\n",
        "        return 1 - input ** 2\n",
        "    return np.tanh(input)\n",
        "\n",
        "def softmax(input):\n",
        "     return np.exp(input) / np.sum(np.exp(input))"
      ],
      "metadata": {
        "id": "BFy9T5nHZIAy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tanh Derivative Definition  \n",
        "$$  \n",
        "\\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x)  \n",
        "$$  \n",
        "\n",
        "### Sigmoid Derivative Definition  \n",
        "$$  \n",
        "\\frac{d}{dx} \\sigma(x) = \\sigma(x)(1 - \\sigma(x))  \n",
        "$$"
      ],
      "metadata": {
        "id": "MsQg9UGgZ85r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Long short-term memory network class\n",
        "class LSTM:\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
        "\n",
        "         ## hyperparameters\n",
        "         self.learning_rate = learning_rate\n",
        "         self.hidden_size = hidden_size\n",
        "         self. num_epochs = num_epochs\n",
        "\n",
        "        ##### weight and baises for the gates\n",
        "\n",
        "         ## forget gate\n",
        "         self.wf = initWeights(input_size, hidden_size)\n",
        "         self.bf = np.zeros((hidden_size, 1))\n",
        "\n",
        "         ## input gate\n",
        "         self.wi = initWeights(input_size, hidden_size)\n",
        "         self.bi = np.zeros((hidden_size, 1))\n",
        "\n",
        "         ## candidate gate\n",
        "         self.wc = initWeights(input_size, hidden_size)\n",
        "         self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "         ## output gate\n",
        "         self.wo = initWeights(input_size, hidden_size)\n",
        "         self.bo = np.zeros((hidden_size, 1))\n",
        "\n",
        "         ## final gate\n",
        "         self.wy = initWeights(hidden_size, output_size)\n",
        "         self.by = np.zeros((output_size, 1))\n",
        "\n",
        "    def reset(self):\n",
        "        self.concat_inputs = {}\n",
        "\n",
        "        self.hidden_states = {-1:np.zeros((self.hidden_size, 1))}\n",
        "        self.cell_states = {-1:np.zeros((self.hidden_size, 1))}\n",
        "\n",
        "        self.activation_outputs = {}\n",
        "        self.candidate_gates = {}\n",
        "        self.forget_gates = {}\n",
        "        self.output_gates = {}\n",
        "        self.input_gates = {}\n",
        "        self.outputs = {}\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.reset()\n",
        "\n",
        "        outputs = []\n",
        "        for r in range(len(inputs)):\n",
        "            self.concat_inputs[r] = np.vstack((self.hidden_states[r - 1], inputs[r]))\n",
        "\n",
        "            self.forget_gates[r] = sigmoid(np.dot(self.wf, self.concat_inputs[r]) + self.bf)\n",
        "            self.input_gates[r] = sigmoid(np.dot(self.wi, self.concat_inputs[r]) + self.bi)\n",
        "            self.candidate_gates[r] = tanh(np.dot(self.wc, self.concat_inputs[r]) + self.bc)\n",
        "            self.output_gates[r] = sigmoid(np.dot(self.wo, self.concat_inputs[r]) + self.bo)\n",
        "\n",
        "            self.cell_states[r] = self.forget_gates[r] * self.cell_states[r - 1] + self.input_gates[r] * self.candidate_gates[r]\n",
        "            self.hidden_states[r] = self.output_gates[r] * tanh(self.cell_states[r])\n",
        "\n",
        "            outputs += [np.dot(self.wy, self.hidden_states[r]) + self.by]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def backward(self, errors, inputs):\n",
        "        d_wf, d_bf = 0, 0\n",
        "        d_wi, d_bi = 0, 0\n",
        "        d_wc, d_bc = 0, 0\n",
        "        d_wo, d_bo = 0, 0\n",
        "        d_wy, d_by = 0, 0\n",
        "\n",
        "        dh_next, dc_next = np.zeros_like(self.hidden_states[0]), np.zeros_like(self.cell_states[0])\n",
        "        for r in reversed(range(len(inputs))):\n",
        "            error = errors[r]\n",
        "\n",
        "            ## final gate weights and biases errors\n",
        "            d_wy += np.dot(error, self.hidden_states[r].T)\n",
        "            d_by += error\n",
        "\n",
        "            \"\"\"for each time step `r`, calculated the gradients for the output\n",
        "             weights d_wy and baises d_by\n",
        "\n",
        "                -> d_wy += np.dot([0.01], [h2].T) hidden state from time step 2\n",
        "            \"\"\"\n",
        "\n",
        "\n",
        "            ## hidden state error\n",
        "\n",
        "            d_hs = np.dot(self.wy.T, error) + dh_next\n",
        "\n",
        "            \"\"\" calculating the propogated error to the hidden state\n",
        "            \"\"\"\n",
        "            ## output gate weights and biases errors\n",
        "\n",
        "            d_o = tanh(self.cell_states[r]) * d_hs * sigmoid(self.output_gates[r], derivative=True)\n",
        "            d_wo += np.dot(d_o, inputs[r].T)\n",
        "            d_bo += d_o\n",
        "\n",
        "\n",
        "            # cell state error\n",
        "            d_cs = tanh(tanh(self.cell_states[r], derivative=True) * self.output_gates[r] * d_hs + dc_next)\n",
        "\n",
        "            # forget gate weights and biases errors\n",
        "            d_f = d_cs * self.cell_states[r - 1] * sigmoid(self.forget_gates[r], derivative=True)\n",
        "            d_wf += np.dot(d_f, inputs[r].T)\n",
        "            d_bf += d_f\n",
        "\n",
        "            ## input gate weights and biases error\n",
        "                        # Input Gate Weights and Biases Errors\n",
        "            d_i = d_cs * self.candidate_gates[r] * sigmoid(self.input_gates[r], derivative = True)\n",
        "            d_wi += np.dot(d_i, inputs[r].T)\n",
        "            d_bi += d_i\n",
        "\n",
        "            ## candidate Gate Weights and Biases Errors\n",
        "            d_c = d_cs * self.input_gates[r] * tanh(self.candidate_gates[r], derivative = True)\n",
        "            d_wc += np.dot(d_c, inputs[r].T)\n",
        "            d_bc += d_c\n",
        "\n",
        "            ## concatenated Input Error (Sum of Error at Each Gate!)\n",
        "            d_z = np.dot(self.wf.T, d_f) + np.dot(self.wi.T, d_i) + np.dot(self.wc.T, d_c) + np.dot(self.wo.T, d_o)\n",
        "\n",
        "            ## error of Hidden State and Cell State at Next Time Step\n",
        "            dh_next = d_z[:self.hidden_size, :]\n",
        "            dc_next = self.forget_gates[r] * d_cs\n",
        "\n",
        "        for d_ in (d_wf, d_bf, d_wi, d_bi, d_wc, d_bc, d_wo, d_bo, d_wy, d_by):\n",
        "            np.clip(d_, -1, 1, out = d_)\n",
        "\n",
        "        self.wf += d_wf * self.learning_rate\n",
        "        self.bf += d_bf * self.learning_rate\n",
        "\n",
        "        self.wi += d_wi * self.learning_rate\n",
        "        self.bi += d_bi * self.learning_rate\n",
        "\n",
        "        self.wc += d_wc * self.learning_rate\n",
        "        self.bc += d_bc * self.learning_rate\n",
        "\n",
        "        self.wo += d_wo * self.learning_rate\n",
        "        self.bo += d_bo * self.learning_rate\n",
        "\n",
        "        self.wy += d_wy * self.learning_rate\n",
        "        self.by += d_by * self.learning_rate\n",
        "\n",
        "\n",
        "    def train(self, inputs, labels):\n",
        "        inputs = [oneHotEncode(input) for input in inputs]\n",
        "\n",
        "        for _ in tqdm(range(self.num_epochs)):\n",
        "            predictions = self.forward(inputs)\n",
        "\n",
        "            errors = []\n",
        "            for r in range(len(predictions)):\n",
        "                errors += [-softmax(predictions[r])]\n",
        "                errors[-1][char_to_idx[labels[r]]] += 1\n",
        "            self.backward(errors, self.concat_inputs)\n",
        "\n",
        "    def test(self, inputs, labels):\n",
        "        accuracy = 0\n",
        "        probabilities = self.forward([oneHotEncode(input) for input in inputs])\n",
        "\n",
        "        output = ''\n",
        "        for  r in range(len(labels)):\n",
        "            predictions = idx_to_char[np.random.choice([*range(char_size)], p=softmax(probabilities[r].reshape(-1)))]\n",
        "            output += predictions\n",
        "\n",
        "            if predictions == labels[r]:\n",
        "                accuracy += 1\n",
        "\n",
        "        print(f'Ground Truth:\\nt{labels}\\n')\n",
        "        print(f'Predictions:\\nt{\"\".join(output)}\\n')\n",
        "        print(f'Accuracy : {round(accuracy*100/len(inputs), 2)}%')\n",
        ""
      ],
      "metadata": {
        "id": "pD25WKKOZH-T"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 25\n",
        "\n",
        "lstm  = LSTM(input_size = char_size + hidden_size, hidden_size=hidden_size, output_size=char_size, num_epochs=1_000, learning_rate=0.05)\n",
        "lstm.train(train_x, train_y)\n",
        "lstm.test(train_x, train_y)"
      ],
      "metadata": {
        "id": "LQ5vO6qgZH7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865ea006-ce7f-4cfa-b40d-04a1e3c85d56"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [02:12<00:00,  7.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth:\n",
            "to be, or not to be, that is the question: whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and by opposing end them. to die—to sleep, no more; and by a sleep to say we end the heart-ache and the thousand natural shocks that flesh is heir to: 'tis a consummation devoutly to be wish'd. to die, to sleep; to sleep, perchance to dream—ay, there's the rub: for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause—there's the respect that makes calamity of so long life. for who would bear the whips and scorns of time, th'oppressor's wrong, the proud man's contumely, the pangs of dispriz'd love, the law's delay, the insolence of office, and the spurns that patient merit of th'unworthy takes, when he himself might his quietus make\n",
            "\n",
            "Predictions:\n",
            "to bg  or tat 'l by, nhat az 'ha suestoon: wea,lepestimhaos' r onewoarhang tormlrfirctoe phiel  oni oroscs sr dldrmnemutaforohrds tf'to shye aons orains  arweatif thonsler a d by ifpo  nesesd the   to limnsh wlfep, marniwon bls te amm en, oh sly,wo.hnd shenmelyc at', snd toe sheusond tothmai shezas aiet mae ,;td mems wo  'tma a lon   fat ncgd robtl' ti by aith'r shordie, hh sceep, to bleens tasasa cs—io bmeam—ay, dha e'c the sub: sortnaschet mleep,tf te tyewhan mhoums moyeco sr whaetto oaveathuselellhff thishcartimochns, taro name ss aacs ata  e-s tie semi  t ahit takes,cokymisy affdh song din . fofepdapweuse ae,i soo soils ata shorcs or dhse— thasp:dtssor's ar ugg taeanrsfd iangs worthme,i, toa mrngs of tisppoc't cone, mheepan's fe aw, tha ems feels or off ssl ond boe wqur t lhet iatam t fsrhshnr toarsw vth' tuye , thedete oaie t,stfrh  his tueee s make\n",
            "\n",
            "Accuracy : 50.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, it’s not going to provide 99.9% accuracy, but it’s just for learning purposes! :)"
      ],
      "metadata": {
        "id": "hJF-ZUa7RxbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Bjiw9PbeTIuV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}